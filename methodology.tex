\section{Methodology}

%\input{methodology-classification}

%\input{methodology-regression}

\subsection{Data preprocessing}


The dataset consists in a bit more than 3'000'000 images collected during 80 surveys between the second half of 2013 up to the end of 2015. We consider two experimental setups : a classification task and a regression task. For the classification task, a label is affected to each image based on the pose of the robot. The pose, consisting in the position (from the GPS) and the heading (from the compass) of the robot, is discretized. The position of the robot is discretized into 2.5 meters squares positioned around the lake in non overlapping angular sectors \todo{how are the blocks positioned ?}. The heading is discretized every 10 degrees. This led to a total of 1300 \todo{how many?} classes. Some of the obtained classes were underrepresented and therefore the dataset was sub-sampled to ensure the balance of the classes. Namely, we kept only the classes with a number of images at least 50\% of the largest class, containing 1750 images. This represents a total of 295 classes with approximately 1'000 images for most classes for a total of 300'000 images on the testing set \todo{what is roughly the distribution of the sizes of the classes ? min and max bounds ?} and 5000 images selected randomly for the train set\todo{how is the dataset split in training and test set ?}. For the regression task, the same set of images was used and the labels were defined from the pose of the robot. One possibility would have been to use the position and heading of the robot as labels but this would imply to define a specific loss taking into account the angular nature of the heading. We rather used an Euclidean loss and therefore defined the labels as a fourth component vector with the position of the robot (from the GPS) and the position of the point 10 meters away from the robot along the optical axis of the camera\todo{is that true?}. One potential drawback of this approach is that the regression problem becomes more complicated than if we were to predict the position and heading since it requires the regressor to predict a specific location along the heading. However it turns out that despite this constraint, the regressor performed reasonably well. Finally, in order to ease the definition of the learning rate and to speed up convergence of learning, the labels to be regressed are normalized and centered.


\subsection{Convolutional neural network architecture and training}

In this study, we used the AlexNet convolutional neural architecture \cite{NIPS2012_4824} trained in Caffe \cite{jia2014caffe}. The network consists in five convolution/pooling layers followed by three fully connected layers\todo{is it true?}. Few modifications were brought and required to train successfully the AlexNet network. For the classification task, the size of the mini-batches is decreased to 32 samples and learning rate was decreased at a regular rate 10 times throughout training. \todo{how is it changed?}. For the classification task, the output layer of the fully connected part of the architecture uses a softmax transfer function to get a probability distribution over the labels and the loss is the cross-entropy classification loss. For the regression task, a linear output transfer function is considered and the loss is the Euclidean loss. Experimentally, it was required to consider a lower learning rate than AlexNet which otherwise lead to a divergent loss. As we shall see in the result section, several strategies for setting the learning rate are considered. For both the regression and classification problems, the architecture is trained with the default CaffeNet settings, namely stochastic gradient descent\todo{what is the algorithm used?}, a momentum set to \todo{??} and a weight decay to \todo{??}, and \todo{??}.


\subsection{Extracting season invariant representations}

Being able to classify an image as belonging to one part of the lake with its viewpoint or to regress from it the pose of the boat is of interest by themselves. However, one of the objectives of the study was also to extract season invariant representations in order to detect the changes of the lake shore. This part of the study was done only from the network trained in the classification task. For every class of the selected dataset, a prototypical image was computing by averaging all the filter responses, at a given depth of the network, of all the images belonging to the considered class. A query image is then propagated through the network up to the depth where the prototypes have been computed, the responses of all the filters at that depth are then averaged and this representation is compared to the computed prototypes. The quality of the computed prototypical images is then assessed from the cosine similarity between the representation of the query image and the prototypes; labelling a new image is then performed by picking the class whose prototype has the largest similarity with the averaged representation of an image.
