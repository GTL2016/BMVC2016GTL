\section{Methodology}

%\input{methodology-classification}

%\input{methodology-regression}

\subsection{Data preprocessing}


The dataset consists in a bit more than 3'000'000 images collected during 80 surveys between the second half of 2013 up to the end of 2015. We consider two experimental setups : a classification task and a regression task. For the classification task, a label is affected to each image based on the pose of the robot. The pose, consisting in the position (from the GPS) and the heading (from the compass) of the robot, is discretized. The position of the robot is discretized into 2.5 meters square \todo{how are the blocks positioned ?}. The heading is discretized every 10 degrees. This led to a total of \todo{how many?} classes. Some of the obtained classes were underrepresented and therefore the dataset was sub-sampled to ensure the balance of the classes. Namely, we kept only the classes with a number of images at least 50\% of the largest class. This represents a total of 295 classes with approximately 1'000 images per class for a total of around 300'000 images \todo{what is roughly the distribution of the sizes of the classes ? min and max bounds ?}. \todo{are the images preprocessed in some ways? resized ? rescaled ?}.For the regression task, the same set of images was used and the labels were defined from the pose of the robot. One possibility would have been to use the position and heading of the robot as labels but this would imply to define a specific loss taking into account the angular nature of the heading. We rather used an Euclidean loss and therefore defined the labels as a fourth component vector with the position of the robot (from the GPS) and the position of the point 10 meters away from the robot along the optical axis of the camera\todo{is that true?}. One potential drawback of this approach is that the regression problem becomes more complicated than if we were predict the position and heading since it requires the regressor to predict a specific location along the heading. However it turns out that despite this constraint, the regressor performed reasonably well. Finally, in order to ease the definition of the learning rate and to speed up convergence of learning, the labels to be regressed are normalized and centered.


\subsection{Convolutional neural network architecture and training}

In this study, we used the AlexNet convolutional neural architecture \cite{NIPS2012_4824} trained in Caffe \cite{jia2014caffe}.


\subsection{Extracting season invariant representations}
