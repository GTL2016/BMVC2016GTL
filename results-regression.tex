\subsection{Regression}
The objective of the regression task is to perform localization with performance comparable to GPS system. The parameters of the training had to be tuned for the Network to perform at its best for the regression task.

The layer of the network are initialized using normal distributions. The initialization variances had to be changed so that the weights are big enough to propagate information through the Network while still ensuring convergence.
The initial learning rate and its evolution policy heavily depends on the loss used. For the regression task the initial learning rate was limited by the initial loss.

Based on those general settings, we tested three different approaches. The first one consists in testing those parameters uniformly among the network. The second one consists in fine-tuning the learning rate of each layer differently. The third one consists in loading the weights of the convolution layers from the classification training. In order to compare our results with those three approaches, we refer to the following loss function :

\begin{equation} 
Loss = \frac{0.5}{scale^{2}}*\sum_{i=1,\ 4}(label_{i}-prediction_{i})^{2}
\end{equation}

The first approach allowed to achieve an average error of 19.3 meters per label as it is shown in the \ref{1apploss}. However after the training, the convolution layers filters did not exhibit any specific features (\ref{1appfilter}). Thus, it appears that the regression was only supported by the fully connected layers. In our case where the goal is to build a seasonal invariant representation of natural scenes this approach did not reach our expectations. 

The second approach led us to consider the difference of behavior of the fully-connected layers regarding the sensitivity to learning rate. By decreasing the learning rate of those layers, we forced the convolution layers to take part in the regression. This method resulted in being more successful than the previous one. The best average error we achieved was 18.3 meters (\ref{2apploss}). Some natural environment features can be retrieved in the convolution filters (\ref{2appfilter}).

The last approach was used on the first approach configuration. It was observed that the convolution weights decreased during the training and ended with a similar distribution than the previous approach. The best average error achieved was 20.9 (\ref{3apploss}). However the convolution filters retrieved exhibited different structures than the previous approach(\ref{3appfilter}). Based on this result it can be concluded that the convolution weights learned during the classification training could not be reused for the regression task. The weights needed for this task required a smaller variance and the final model presented the worst results among the three approaches. Consequently, learning from scratch seems to be more efficient in the case of a regression.


\begin{figure}[!h]
\centering
\begin{subfigure}{0.33\textwidth}
\label{1apploss}
\centering
\includegraphics[width=0.9\linewidth]{images/regression/test_loss_26_135000.png}
\caption{First approach}
\end{subfigure}%
\begin{subfigure}{0.33\textwidth}
\label{2apploss}
\centering
\includegraphics[width=0.9\linewidth]{images/regression/test_loss_37_135000.png}
\caption{Second approach}
\end{subfigure}%
\begin{subfigure}{0.33\textwidth}
\label{3apploss}
\centering
\includegraphics[width=0.9\linewidth]{images/regression/test_loss_30_135000.png}
\caption{Third approach}
\end{subfigure}
\caption{Loss Function for 135 000 iterations}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}{0.33\textwidth}
\label{1appfilter}
\centering
\includegraphics[width=0.9\linewidth]{images/regression/conv1_26_135000.png}
\caption{First approach}
\end{subfigure}%
\begin{subfigure}{0.33\textwidth}
\label{2appfilter}
\centering
\includegraphics[width=0.9\linewidth]{images/regression/conv1_37_135000.png}
\caption{Second approach}
\end{subfigure}%
\begin{subfigure}{0.33\textwidth}
\label{3appfilter}
\centering
\includegraphics[width=0.9\linewidth]{images/regression/conv1_30_135000.png}
\caption{Third approach}
\end{subfigure}
\caption{Conv1 filters after 135 000 iterations}
\end{figure}

The best results in term of generalization and precision of the prediction were achieved with the second approach. After 148000 iterations, the prediction errors are centered Gaussian-like distributions with a standard deviation of 12.4 meters on X and 20.6 meters on Y. Considering that performing this operation with the human brain is found to be challenging for image from a natural environment, the resulting precision is very acceptable. The labels were displayed to be compared to the predicted positions (\ref{map}).

\begin{figure}[!h]
\centering
\begin{subfigure}{0.5\textwidth}
\centering
\label{map}
\includegraphics[width=0.7\linewidth]{images/regression/map.png}
\caption{Map of predicted positions and original labels}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\centering
\label{lake}
\includegraphics[width=0.7\linewidth]{images/regression/lake.png}
\caption{Map of predicted positions and original labels}
\end{subfigure}
\end{figure}